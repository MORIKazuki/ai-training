# 第3章 ニューラルネットワーク

## ソフトマックス関数について
ソフトマックス (softmax) は多クラス分類の出力層でよく使われる関数で、スコア（実数ベクトル）をクラスごとの確率分布に変換します。

定義（ベクトル z に対して）:

```
softmax(z)_i = exp(z_i) / sum_j exp(z_j)
```

主な性質と注意点:
- 出力は 0〜1 の値を取り、全要素の和は 1 になります（確率分布として解釈可能）。
- 数値安定性: 入力の最大値を引く (z - max(z)) としてから exp を取ることでオーバーフローを防ぎます。
- 損失関数との組合せ: 一般にクロスエントロピー損失（cross-entropy）と組み合わせて使われます。直接 log を取る場合は log-sum-exp を使うのが安全です。

用途例: 出力層でクラス確率を求め、最も高い確率のインデックスを予測ラベルとします。

### 採用の経緯・理由・その後

ソフトマックスは多クラスロジスティック回帰（multinomial logistic regression）で自然に使われてきた手法で、ニューラルネットワークにおいては出力を確率として解釈したい場合に標準的に採用されました。クロスエントロピー損失と組み合わせることで、理論的に整合した確率的学習が可能になります。

その後の発展としては、数値安定化（max を引く手法）や温度付きソフトマックス（temperature scaling）による出力分布の鋭さ調整、さらに確率分布のスパース化を目的とした sparsemax などの代替手法が提案されています。分類以外では注意重み（attention）の正規化などにも広く使われています。

## シグモイド関数について
シグモイド (sigmoid) は古典的な活性化関数の一つで、入力を (0,1) の範囲にマッピングします。

定義:

```
sigmoid(x) = 1 / (1 + exp(-x))
```

主な性質と注意点:
- 出力範囲は (0, 1)。確率的解釈を行う層などで使われます（ただし出力層に softmax を使う多クラス分類では通常 softmax を使います）。
- 微分は sigmoid(x) * (1 - sigmoid(x)) で計算でき、逆伝播で使われます。
- 飽和（saturation）問題: 入力が大きく正または負に振れると勾配が非常に小さくなり、学習が遅くなることがあります（勾配消失）。
- 数値的注意: exp(-x) の計算でオーバーフロー/アンダーフローが発生する可能性があるため、必要なら入力をクリップするなどの工夫を行います。

近年は ReLU 系（後述）の方が深いネットワークで扱いやすいため中間層ではそちらがよく使われますが、出力に確率を出したい場合など限定的な用途で使われます。

### 採用の経緯・理由・その後

シグモイド（ロジスティック関数）は、統計学のロジスティック回帰から派生してニューラルネットの初期から出力および隠れ層の活性化関数として広く使われてきました。採用理由は、滑らかで微分可能、出力を確率的に解釈できる点にあります。

しかし深いネットワークでは活性化が飽和して勾配が消失しやすく、学習が進みにくいという問題（勾配消失）が指摘されました。そのため隠れ層では tanh や後の ReLU 系に置き換えられることが多くなりました。出力層では二値分類に対して今も sigmoid が使われます。

近年は学習の安定化や性能向上を目的に、Swish, Mish, GELU といった新しい活性化関数が提案され、特定のアーキテクチャ（例: Transformer）では GELU が標準となるなどの変化が見られます。

## ReLU関数について
ReLU (Rectified Linear Unit) は現在もっとも広く使われている活性化関数の一つで、計算が簡単で学習が速いという利点があります。

定義:

```
ReLU(x) = max(0, x)
```

主な性質と注意点:
- 正の入力はそのまま通し、負の入力はゼロにします。これによりスパースな活性化が得られ、表現学習が効率的になることがあります。
- 勾配の伝播が比較的良好で、深いネットワークで学習が進みやすいという利点があります。
- "Dying ReLU" 問題: 学習率などの条件によってはニューロンが常に負側に入り、勾配が 0 になって更新されなくなることがあります。これを回避するために Leaky ReLU や Parametric ReLU などの変種が使われることがあります。

用途例: 畳み込み層や全結合層の中間活性化として標準的に使われます。

### 採用の経緯・理由・その後
ReLU は比較的新しい関数で、Nair と Hinton（2010 年頃）の研究などを通じて深いネットワークでの学習を容易にすることが示され、2010年代に急速に普及しました。主な採用理由は計算が単純で非飽和領域を持つため勾配消失問題が緩和され、スパースな表現が得られる点です。

採用後は "Dying ReLU" の問題に対処するために Leaky ReLU、Parametric ReLU (PReLU)、Exponential Linear Unit (ELU) といった改良版が登場しました。また、バッチ正規化（BatchNorm）や重み初期化の改善と組み合わせることで、ReLU ベースの大規模モデルが安定して学習できるようになりました。

さらに最近では、GELU や Swish のような滑らかな活性化関数が Transformer や一部の CNN で採用されるなど、用途に応じて最適な活性化関数を選択する流れが定着しています。

## MNSIT
このディレクトリにある `neuralnet_mnist.py` は、事前学習済みの重みを使って MNIST データセットのテストデータに対する分類精度を評価するためのシンプルなスクリプトです。

### 目的
- 事前学習済みパラメータ（`sample_weight.pkl`）を読み込み、MNIST のテストセットを用いて順伝播（推論）を行い、分類精度（Accuracy）を算出します。

### ファイル内の主要関数

- `get_data()`
	- MNIST データセットを読み込み、テスト用の入力 `x_test`（形状 (N, 784)）とラベル `t_test`（一次元配列）を返します。
	- 内部で `load_mnist(normalize=True, flatten=True, one_hot_label=False)` を呼び出します。

- `init_network()`
	- カレントワークディレクトリにある `sample_weight.pkl` を開いて、重みとバイアスを含む辞書 `network` を読み込みます。
	- `network` は `'W1','W2','W3','b1','b2','b3'` のようなキーを持つことを前提としています。

- `predict(network, x)`
	- 単一の入力ベクトル `x` に対して順伝播を実行します。
	- 中間層の活性化は `sigmoid`、出力層には `softmax` を使い、確率ベクトルを返します（例: 10 クラスの場合は長さ 10 のベクトル）。
	- 現状は単一入力向け（1D の `x`）の実装です。バッチ処理（複数入力）に対応させるには行列演算に合わせて修正してください。

### 実行前の準備
1. MNIST データが利用できること（`deep_learning/data` に適切なデータがあるか、`dataset/mnist.py` がダウンロード処理を持っているかを確認）。
2. 事前学習済みパラメータ `sample_weight.pkl` が `ch03` のカレントディレクトリまたはスクリプト実行時のカレントディレクトリに配置されていること。

### 実行方法（Docker コンテナ内）
リポジトリに同梱の `docker-compose` を使う場合の最短手順:

```powershell
# コンテナ内でスクリプトを実行
docker exec -it deep_learning_training /bin/bash -lc "python /app/workspace/ch03/neuralnet_mnist.py"
```

### 期待される出力
- スクリプトはネットワーク辞書を `print()` で表示した後、最終的に Accuracy を表示します。（※実際の値は `sample_weight.pkl` の品質とデータの前処理に依存します）

```
Accuracy:0.9352
```

## 前処理と正規化について

 `neuralnet_mnist.py`では `load_mnist(normalize=True, flatten=True, one_hot_label=False)` を用いています。ここでの「正規化（normalize）」とその理由、一般的な前処理について説明します。

- 正規化 (Normalization)
	- `normalize=True` にすると、MNIST の画素値（元は 0〜255 の整数）は 0.0〜1.0 の実数にスケーリングされます。
	- 理由: ニューラルネットワークの学習や推論では、入力のスケールを揃えることで勾配計算の安定性が向上し、収束が速くなることが多いです。活性化関数（sigmoid や tanh など）は入力が大きいと飽和しやすく、正規化はこれを緩和します。

- 平滑化や中心化（オプション）
	- より進んだ前処理として平均値を引いてゼロ中心化（zero-centering）したり、標準偏差で割ってスケーリング（標準化）する方法があります。例えば画像全体の平均と標準偏差を使って (x - mean) / std を行う手法です。
	- これらはデータセットやモデルにより有効性が変わりますが、畳み込みニューラルネットワーク（CNN）やバッチ正規化を使う場合にも一般的に有効です。

- 画像の型と dtype
	- `normalize=True` を使うと float 系の配列になります。モデルの重みと計算の精度に応じて `float32` を明示するとメモリと計算速度のバランスが良くなります。

- 実践上の注意点
	- 学習時と推論時で同じ前処理を必ず行ってください（学習時に行った標準化のパラメータを保存しておき、推論時に同じ値で復元する）。
	- 外れ値や欠損がある場合は事前に処理してください（MNIST では通常不要）。
	- ラベルの表現（ワンホットか整数か）によって損失関数の選択が変わります（例: one-hot ラベルなら categorical_crossentropy、整数ラベルなら sparse_crossentropy に相当）。