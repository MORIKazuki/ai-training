# 第4章 ニューラルネットワークの学習

## 特徴量
特徴量は、データの性質を数値ベクトルとして表したもので、学習や推論の“入力材料”になるもの。
より形式的には、生データを変換して得られ、後続タスクに使われる表現（representation）のこと。

- 自動的に特徴を学習：従来の機械学習では人手で「特徴量エンジニアリング」をして入力を工夫するが、ディープラーニングは多層のネットワークがデータから表現（特徴）を自動学習する（＝表現学習/representation learning）。
- 層が深くなるほど抽象的に：たとえばCNNでは、浅い層がエッジや色など低次の特徴、深い層がテクスチャや部品、最終的に物体レベルの特徴を表す、といった階層的特徴が形成されることが可視化研究で示されている。

以下の違いがある。

- 入力→ヒトの考えた特微量(SIFH、HOGなど)→機械学習(SVM、KNN)→答え
- 入力→ニューラルネットワーク(ディープラーニング)→答え

## 損失学習
ニューラルネットワークの学習は、モデルが出力した予測と正解ラベルの“ずれ”を表す損失関数（loss function）を定義し、その損失を最小化するようにパラメータ（重み・バイアス）を更新するプロセスです。本節では基礎概念と実践上のポイントをまとめます。

- 損失関数の役割
    - モデルの予測誤差を数値化します。学習はこの損失を最小化する最適化問題として定式化されます。
    - 代表例:
        - 二乗誤差 (MSE): 回帰問題でよく使われる。損失 = 1/N * sum (y_pred - y_true)^2。
        - 交差エントロピー (cross-entropy): 分類問題で標準的。確率出力（softmax）と組み合わせると理論的に自然です。

- 最適化（オプティマイザ）
    - 損失を最小化するために勾配を使ってパラメータを更新します（勾配降下法: Gradient Descent）。
    - 実務でよく使われる手法:
        - SGD（確率的勾配降下法）: 全データではなくミニバッチ単位で勾配を計算。
        - Momentum, Nesterov: 局所振動の抑制と収束加速。
        - Adam, RMSprop, Adagrad: 学習率を自動調整する手法で多くの問題で安定する。

- ミニバッチ学習とエポック
    - ミニバッチ（バッチサイズ）を設定してデータを分割し、1 バッチごとに勾配更新を行うことで計算効率と収束性を両立します。
    - 1 エポックは全データを一度見終わることを指します。複数エポックにわたって学習します。

- 学習率（learning rate）とハイパーパラメータ
    - 学習率は最も重要なハイパーパラメータの一つ。大きすぎると発散、小さすぎると収束が遅い。
    - 学習率スケジューラ（減衰、ステップ減衰、Cosine Annealing など）を使うと性能が向上することがあります。

- 正則化と過学習対策
    - 過学習（訓練データに過度に適合し汎化性能が落ちる）を避けるために正則化を行います。
    - 代表的手法: L2/L1 正則化（重み減衰）、ドロップアウト、データ拡張。

- 勾配に関する実践上の注意
    - 勾配消失・勾配爆発: 深いネットワークでは勾配が消えたり発散したりする問題がある。活性化関数の選択（ReLU 系）やバッチ正規化、適切な重み初期化が有効です。
    - 勾配クリッピング: 勾配爆発を防ぐために勾配のノルムを制限する手法。

- 評価と早期停止
    - 学習時は訓練データと検証データに分け、検証損失が改善しなくなったら学習を打ち切る（早期停止）ことで過学習を抑制します。

- 実装のチェックポイント
    - 学習ループでは以下を毎エポック/毎ステップ確認すると良い:
        - 訓練損失・検証損失の推移
        - 精度などの評価指標
        - 学習率や勾配の分布（異常がないか）

まとめ: 損失学習は損失関数の選択、最適化手法、ハイパーパラメータの調整、正則化戦略などの組合せで性能が決まります。小さなモデルや問題で基本を確かめた上で、オプティマイザや正則化を段階的に導入してください。

### 二乗和誤差（二乗誤差 / MSE）について

二乗和誤差（ここでは平均二乗誤差 Mean Squared Error, MSE と同義で記述します）は、予測と正解の差の二乗を平均した損失で、主に回帰問題で用いられます。式で表すと:

```
MSE = (1/N) * sum_{i=1..N} (y_pred_i - y_true_i)^2
```

```python
def mean_squared_error(y ,t):
    return 0.5 * np.sum((y -t)**2)
```

主な特徴と実践上の注意点:
- 平滑で二階微分可能（ニューラルネットワークの勾配計算に適する）。
- 線形モデルに対しては凸関数で解析的解が得られる場合があるが、ニューラルネットワークでは非凸最適化になります。
- 外れ値に敏感: 誤差を二乗するため、大きな誤差が損失に強く影響します。外れ値が問題になる場合は MAE（平均絶対誤差）や Huber 損失が検討されます。
- スケーリング: データのスケールに依存するため、入力と出力のスケーリング（正規化・標準化）に注意すると学習が安定します。
- 確率的解釈: MSE を最尤推定の観点から見ると、出力誤差が平均 0、分散一定の正規分布に従うという仮定に相当します。

一般に回帰タスクのデフォルトな選択肢ですが、データ特性に応じて損失を選ぶことが重要です。

### 交差エントロピー誤差
交差エントロピー誤差（cross-entropy error, CEE）は分類問題でよく使われる損失関数で、予測確率と正解ラベル（確率分布）の不一致度を測る指標です。確率的な枠組み（尤度最大化）に直接対応するため、分類タスクでは MSE よりも好まれることが多いです。

#### 定義（カテゴリ分類）
正解ラベルがワンホット表現 t (= one-hot vector) で、出力確率ベクトルを y とすると:

```
L = - sum_i t_i * log(y_i)
```

ミニバッチ平均を取る場合はバッチごとの和を平均します。整数ラベル（クラスインデックス） t が与えられる場合は、対応する確率のみを使います:

```
L = - log( y[class_index] )
```

#### バイナリ交差エントロピー
二値分類（シグモイド出力 y, 正解 t in {0,1}）では:

```
L = - ( t * log(y) + (1 - t) * log(1 - y) )
```

#### なぜ分類でよく使われるか
- 交差エントロピーは確率モデル（softmax + categorical cross-entropy）の負の対数尤度に対応し、確率出力に対して理論的に自然です。
- 勾配の形状がクラス確率の誤差に直結するため、学習の収束が速い場合が多いです。

#### 数値安定化
- softmax の後に log を取ると数値的に不安定になることがあるため、多くのライブラリは "softmax と交差エントロピーを合わせて" 計算する専用ルーチン（log-sum-exp を利用）を提供します。
- 単純実装での対策としては、`np.log(y + eps)` のように小さな eps（例 1e-7）を足してゼロ除算や log(0) を防ぎます。

#### 実装例（NumPy）
ワンホットラベルを想定した簡易実装:

```python
import numpy as np

def cross_entropy_error(y, t):
    # y: (N, C) の確率、t: (N, C) のワンホットまたは (N,) の整数ラベル
    if y.ndim == 1:
        y = y.reshape(1, y.size)
        t = t.reshape(1, t.size) if t.ndim == 1 else t.reshape(1)

    batch_size = y.shape[0]
    # 整数ラベルの場合
    if t.ndim == 1:
        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
    # ワンホットラベルの場合
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

#### 発展・注意点
- ラベルスムージング（label smoothing）は過学習や確信度過剰を抑えるために交差エントロピーと組み合わせて使われるテクニックです。
- 出力が確率分布であることが前提なので、softmax とセットでの利用が一般的です。フレームワークでは `softmax_cross_entropy_with_logits` のような結合関数を使うと数値安定かつ効率的です。

交差エントロピーは分類タスクの基本的かつ強力な損失で、実務でも最初に検討すべき選択肢の一つです。

## 参考資料
- [Qiita ゼロから作るDeep Learningで素人がつまずいたことメモ:4章](https://qiita.com/segavvy/items/bdad9fcda2f0da918e7c)
    - TODO:ソフトマックス関数、数値微分を多次元配列に対応させる部分の解説作成
- [ゼロから作るDeep Learning 4章 ニューラルネットワークの学習](https://n3104.hatenablog.com/entry/2017/03/20/165405)